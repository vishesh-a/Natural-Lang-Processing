{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"A cat was chasing the mice. there were cacti round the corner\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "example = [lemmatizer.lemmatize(token) for token in example.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'cat',\n",
       " 'wa',\n",
       " 'chasing',\n",
       " 'the',\n",
       " 'mice.',\n",
       " 'there',\n",
       " 'were',\n",
       " 'cactus',\n",
       " 'round',\n",
       " 'the',\n",
       " 'corner']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nStemming\\nLemmatizer\\nVectorizer\\nSimilarity Measure -\\n    - Manhattan Similarity\\n    - Euclidean Similarity\\n    - Cosine Similarity - X . Y / (||X|| * ||Y||)\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Stemming\n",
    "Lemmatizer\n",
    "Vectorizer\n",
    "Similarity Measure -\n",
    "    - Manhattan Similarity\n",
    "    - Euclidean Similarity\n",
    "    - Cosine Similarity - X . Y / (||X|| * ||Y||)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "d1 = open(\"./1.txt\", \"r\")\n",
    "d2 = open(\"./2.txt\", \"r\")\n",
    "#d3 = open(\"./3.txt\", \"r\")\n",
    "\n",
    "corpus = [d1.read(), d2.read()]\n",
    "\n",
    "d1.close()\n",
    "d2.close()\n",
    "#d3.close()\n",
    "#corpus =[\"tesseract is good optical character recognition \", \"Optical character recognition is significant\", \"Optical recognition is nice and good for\" ,\"the people who love image processing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.05239019,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.18638026,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.11182816,  0.        ,  0.        ,\n",
       "         0.0745521 ,  0.        ,  0.0745521 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.15717057,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.03727605,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.03727605,  0.03727605,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.05239019,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.10478038,  0.        ,\n",
       "         0.        ,  0.        ,  0.14910421,  0.        ,  0.        ,\n",
       "         0.        ,  0.0745521 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.05239019,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.0745521 ,\n",
       "         0.        ,  0.0745521 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.03727605,  0.        ,  0.        ,  0.        ,\n",
       "         0.22365631,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.05239019,\n",
       "         0.        ,  0.14910421,  0.0745521 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.05239019,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.10478038,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.05239019,  0.        ,  0.05239019,\n",
       "         0.        ,  0.        ,  0.        ,  0.03727605,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.03727605,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.26195096,\n",
       "         0.        ,  0.        ,  0.03727605,  0.        ,  0.        ,\n",
       "         0.        ,  0.29820842,  0.        ,  0.        ,  0.        ,\n",
       "         0.0745521 ,  0.        ,  0.        ,  0.        ,  0.11182816,\n",
       "         0.        ,  0.        ,  0.        ,  0.03727605,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.20956076,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.05239019,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.05239019,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.05239019,  0.        ,  0.        ,\n",
       "         0.05239019,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.03727605,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.22365631,  0.44731263,  0.20956076,  0.        ,  0.05239019,\n",
       "         0.03727605,  0.        ,  0.        ,  0.        ,  0.05239019,\n",
       "         0.05239019,  0.05239019,  0.        ,  0.        ,  0.33548447,\n",
       "         0.        ,  0.        ,  0.        ,  0.26093237,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.05239019,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.0745521 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.03727605,  0.        ,\n",
       "         0.10478038,  0.        ,  0.        ,  0.        ,  0.03727605,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.0745521 ,  0.03727605,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform([\"\"\"If there is a phrase I would prefer to retire from online bios, personal or professional it is, \"I love travel\". Or some approximation of that sentiment. To clarify, I am not against ravellers or those who proudly flaunt their passion for travel. On the contrary, editing a travel magazine has now made me oddly protective of travellers and their ilk. My submission is that \"love to travel\", suggested so casually, just doesn't fell adequate to the depth of emotion it sparks in true devotess.  In february, the month of love as endowed by our great gifting industrial complex, we are wrestling with that \"love for travel\" means in tangible, life-affecting terms. The early thrones of discovering travel might not be too dissimilar to the beginnings of a feverish affair. A fleeting scene, sound or feeling that at first arouses, then enchants and eventually, lures us into hypnotic state, evoking woolly-eyed reveries about what could be.\n",
    "This world however is not the most conductive for long-term passion. The kind that demans unflinching sustenance in the midst of distractions from a thousand notifications. Passion has many rivals to content with. and in flippantly announcing travel as our first love, we are not fully considering the influence our other paramours exert on us. Travellers for life are compulsive. They have to be, ther eis no other existence. Climate change, marriages, deaths, protests and politics might have to take a backseat. I am reminded of a wanderlist happy couple that discovered chinks in their relationshipdumped a road trip through RUssia. The woman was distraught, offered a solution, cut the short to come back home and rekindle their chemistry. The man, however,  would not give Siberia, despite a chance at a do-over. Passion dont \"\"\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.6431055   0.70891204]\n",
      " [ 0.6431055   1.          0.63495089]\n",
      " [ 0.70891204  0.63495089  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "d2 = open(\"./2.txt\", \"r\").read()\n",
    "d1 = open(\"./1.txt\", \"r\").read()\n",
    "\n",
    "\n",
    "similarity = cosine_similarity(vect.transform([\"\"\"If there is a phrase I would prefer to retire from online bios, personal or professional it is, \"I love travel\". Or some approximation of that sentiment. To clarify, I am not against ravellers or those who proudly flaunt their passion for travel. On the contrary, editing a travel magazine has now made me oddly protective of travellers and their ilk. My submission is that \"love to travel\", suggested so casually, just doesn't fell adequate to the depth of emotion it sparks in true devotess.  In february, the month of love as endowed by our great gifting industrial complex, we are wrestling with that \"love for travel\" means in tangible, life-affecting terms. The early thrones of discovering travel might not be too dissimilar to the beginnings of a feverish affair. A fleeting scene, sound or feeling that at first arouses, then enchants and eventually, lures us into hypnotic state, evoking woolly-eyed reveries about what could be.\n",
    "This world however is not the most conductive for long-term passion. The kind that demans unflinching sustenance in the midst of distractions from a thousand notifications. Passion has many rivals to content with. and in flippantly announcing travel as our first love, we are not fully considering the influence our other paramours exert on us. Travellers for life are compulsive. They have to be, ther eis no other existence. Climate change, marriages, deaths, protests and politics might have to take a backseat. I am reminded of a wanderlist happy couple that discovered chinks in their relationshipdumped a road trip through RUssia. The woman was distraught, offered a solution, cut the short to come back home and rekindle their chemistry. The man, however,  would not give Siberia, despite a chance at a do-over. Passion dont \n",
    "\"\"\", d2, d1]))\n",
    "print (similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
